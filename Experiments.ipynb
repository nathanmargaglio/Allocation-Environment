{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_disaster\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-Environment Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its simplest form, we need the sub-environment, `Disaster-v0`, to have the following characteristics:\n",
    "\n",
    "- The Sub-Environment should be ran by a Sub-Agent (e.g., A2C)\n",
    "- The env should produce, as observations, information only pertinent to itself (e.g., it's unaware of the other sub-environment instances)\n",
    "- The action the environment accepts should come from the allocation agent\n",
    "\n",
    "In itself, the environment should be self-sustaining as an environment suitable for RL.  It's recieved actions should only be *adjusted* by the allocation agent, not determined.  The associated sub-agent determines the actual action.\n",
    "\n",
    "For our Snow Plow example, the finite resource we need to allocate is available plows.  Each sub-environment represents a region that may require plowing.  Reward is dictated by the accumulation of snow and the number of plows present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Current Plows: 0.0\n",
      "Current Accumulation: 0.0\n",
      "Realized Fall: 0.0\n",
      "Forecasted Mean: 2\n",
      "Forecasted Variance: 0.704732434179131\n",
      "Reward: None\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Disaster-v0')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, policy, verbose=False):\n",
    "    obs = env.reset()\n",
    "    verbose and print(obs)\n",
    "    plows = policy(obs)\n",
    "    verbose and env.render()\n",
    "    verbose and print()\n",
    "    done = False\n",
    "    total_reward = 0.\n",
    "\n",
    "    while not done:\n",
    "        obs, reward, done, info = env.step(plows)\n",
    "        plows = policy(obs)\n",
    "        verbose and print(obs)\n",
    "        verbose and env.render()\n",
    "        verbose and print()\n",
    "\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            verbose and print('Total Reward:', total_reward)\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_policy \t -483.5089631372183\n",
      "triv_policy_0 \t -3416.019265603014\n",
      "triv_policy_1 \t -1078.6756352792497\n",
      "triv_policy_2 \t -448.19018729572\n",
      "triv_policy_3 \t -336.90086011052864\n",
      "triv_policy_4 \t -393.85779280250836\n",
      "triv_policy_5 \t -472.3504074135186\n",
      "triv_policy_6 \t -571.294985683546\n",
      "simple_policy \t -648.9536114880871\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "def test(func):\n",
    "    env = gym.make('Disaster-v0')\n",
    "    global results\n",
    "    results[func.__name__] = {\n",
    "        'data': [],\n",
    "        'func': func\n",
    "    }\n",
    "    for i in range(100):\n",
    "        results[func.__name__]['data'].append(run(env, results[func.__name__]['func']))\n",
    "\n",
    "def random_policy(obs):\n",
    "    return np.random.choice(env.env.total_plows)\n",
    "    \n",
    "def trivial_policy(num):\n",
    "    def tp(obs):\n",
    "        return num\n",
    "    tp.__name__ = f'triv_policy_{num}'\n",
    "    return tp\n",
    "\n",
    "def simple_policy(obs):\n",
    "    plows = obs[0] * env.env.total_plows\n",
    "    accum = obs[1] * env.env.max_accum\n",
    "    mean = obs[2] * env.env.max_fall\n",
    "    var = obs[3] * env.env.max_forecast_var\n",
    "\n",
    "    base = 0\n",
    "    if accum > 10:\n",
    "        return base + 3\n",
    "    elif accum > 5:\n",
    "        return base + 2\n",
    "    elif accum > 0:\n",
    "        return base + 1\n",
    "    else:\n",
    "        if mean < 2:\n",
    "            return base\n",
    "        else:\n",
    "            return base + 1\n",
    "\n",
    "test(random_policy)\n",
    "for num in range(env.env.total_plows+1):\n",
    "    test(trivial_policy(num))\n",
    "test(simple_policy)\n",
    "\n",
    "for key in results:\n",
    "    print(key, '\\t', np.mean(results[key]['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Allocation Considerations\n",
    "\n",
    "Switching gears, we now have to consider the allocation agent's task and how it relates to this sub-agent.  To begin, we consider the simple example where the sub-environment is a simple stock trading environment where the observation space is data pertinent to forecasting the future asset value (relative to it's current value), the action space is a discrete space of two actions (long and neutral), and the reward is a surrogate for the return on investment per time step.\n",
    "\n",
    "At any time step, the sub-agent will observe the data and decide to either be long or neutral in the asset.  If the sub-agent goes long, then the sub-agent will recieve a positive reward if the value of the asset increases during the next time step, the reward will be negative if the value decreases, and the reward will be zero if the asset's value doesn't change.\n",
    "\n",
    "To relate this to the real-world, this task is similar to given someone some amount of cash (say, $100), and having them look at the stock's chart every hour and deciding to either invest all of their available cash in a specific stock XYZ or to hold on to their cash (i.e., if the person thinks the price of XYZ is going to increase in the next hour, they will buy $100 worth of XYZ and reasses their position in an hour).\n",
    "\n",
    "Now, the allocation agent is tasked with allocating resources to sub-agents, and is agnostic to the actual sub-environment and sub-agent it is creating allocations for.  This makes the allocation agent a manager of the sub-agents, forecasting how well a sub-agent will perform (given pertinent observation data) relative to other sub-agents, and allocating resources to maximize it's own reward.\n",
    "\n",
    "In the running example, we could imagine a number of different sub-agent's trading different assets (or possibly even the same asset), with all operating in the same way working to maximize their own reward.  At any time step, one sub-agent might be in a position where they have the potential to gain a greater reward than the other, or one is more likely to be profitable, etc.  Each individual sub-agent is unaware of this: they are not concerned with the operations of the other sub-agents and only attempt to maximize their own reward.  This is where the allocation agent comes in.\n",
    "\n",
    "The allocation agent observes the operations of all the sub-agents (including how well they are performing, how they perform relative to one another, how they perform given specific observations, etc).  The allocation agent then allocates a percentage of the available assets to each sub-agent, based on it's prediction of how well they will do during the time step.  The allocation agent will allocate more resources to agent's it thinks will result in the highest reward, which may change (possibly considerably) after every time step.\n",
    "\n",
    "In our example, the sub-agents may be acting as a team of independent stock traders while the allocation agent is their manager.  The allocation agent has a finite amount of resources (e.g., available cash), and must decide how much cash to give each agent at every point in time in order to maximize profits.  If the manager feels one agent is in a position to perform relatively better than another, the manager will choose to allocate more cash to that agent.  The manager, however, will want to diversify in order to reduce risk (and increase average, accumulated rewards), so the manager will probably not allocate all of it's resoruces to only one agent, and instead will distribute the resources among the agents in a way that would give more resources to agents it forecasts to do better.\n",
    "\n",
    "To simplify matters, the sub-agents operate under the assumption that the control one unit of a continuous resource.  In our example, the sub-agent doesn't actually control any captial; it merely makes suggestions as to whether or not going long would be more profitable than being neutral (i.e., either 0% invested or 100% invested).  The allocation agent is the one that actually determines how much of it's resources are to be allocated to the sub-agent (e.g., if the manager controls \\$1000 and thinks a specific sub-agent will be profitable, they may give the sub-agent 50% of it's captial to trade, or $500)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plowing as an Investment\n",
    "\n",
    "With all this being said, we need to construct our environment to run on similar logic.  That is, each sub-agent needs to be operating in it's own sub-environment, independent and agnostic to one another.  The sub-agent attempts to maximize some reward which can be interpretted as a return on investment during some period of time.  The allocation agent must then determine which sub-agents are more likely to produce the most reward, and allocate more of it's resources to them.  In our case, the finite resource that the allocation agent controls is the number of plows, and each sub-agent is attempting to utilize plows to reduce snow accumulation in a specific region.\n",
    "\n",
    "The analogy to trading assets can be maintained by defining a proper return on investment, which is less natural in this case.  For trading assets (such as stocks), return is easily calculated as the final price of the asset, minus trading costs, divided by the original price (so if at $t_0$ stock XYZ was trading at $100 and an agent goes long, then, if at $t_1$ XYZ is trading at $110 and there is no cost to trade, the return will be 10%).  For plowing snow, we need to formulate these pieces to produce a similar value.\n",
    "\n",
    "An example of how this could work: let's say we have a city split into 4 regions (A, B, C, D), and our goal is to minimize the snow accumulation throughout the city.  We have four plows available, each of which can occupy one region at a time (with the possibility of having multiple plows in a single region).  The plows reduce snow accumulation in the region they are in for the time steps they are present.  Then the naive approach would be to give each region a single plow and call it a day.  This solution, however, may not be optimal.\n",
    "\n",
    "Let's assume that a plow can reduce the average snow accumulation in a region by 1\" per time step.  So, if two plows are in a single region during a time step, they will reduce the average accumulation in that region by 2\".  Now, if we go with the naive approach, then each region will have their snow accumulation reduced by 1\" every time step.  If we value the minmization of snow accumulation in each region equally, and each region recieves the same amount of snow fall per time step, then this solution would be valid (and optimal, albeit trivially). But these assumptions might not hold.\n",
    "\n",
    "Let's say that, at a given time step, region A and B are to recieve 2\" of snow while regions C and D are to recieve 0\".  Then our naive solution would be suboptimal (with the optimal solution being allocating two plows to A and B each).  So we see that the allocation agent's task is non-trivial given more realistic data.  On top of this, the value we place on the minimization of snow accumulation may be different per region and may possibly change per time step: if region A is downtown and region B is a rural area, and if the time is 7:00 AM on a weekday, then reducing the accumulation in region A may be seen as more valuable compared to B given the greater amount of traffic region A would be recieving at that time.\n",
    "\n",
    "Another consideration is the constraints we need to put on the sub-agents in each region.  Trivially, each sub-agent will attempt to allocate 100% of the plows to their region, if they are not experiencing any accumulation as this would be the most \"profitable\" approach regardless of the observation.  This, of course, would not work in real life.  In reality, we have a finite number of plows that need to be properly distributed between regions, so we certainly couldn't make all the regions happy all the time.  To counter this, we can associate a cost with allocating plows to their region, with the idea being that a sub-agent will have to balance the cost of the plow and the \"profit\" of the snow removal so as to \"take what they only need.\"\n",
    "\n",
    "This balancing is, in itself, not a trivial task and will require a fair amount of analysis.  But, assuming we can properly formulate this cost, we can have the sub-agents allocating an optimal number of plows for their region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we can make simple policies to make decisions about how many plows to allocate to the region, but with the environment potentially being complex, this task becomes time consuming and computationally expensive to perform.  We instead train an Actor-Critic agent to learn to maximize the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curls.framework import SessionManager, ActorCriticAgent, get_discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActorCriticAgent(env, batch_size=64)\n",
    "session = SessionManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -502.5765446158387\n",
      "1 -378.70139825609573\n",
      "2 -531.7063309895989\n",
      "3 -503.9768079730031\n",
      "4 -436.25307849886156\n",
      "5 -407.5726192294591\n",
      "6 -361.0140298146468\n",
      "7 -639.784779290138\n",
      "8 -395.92667476232833\n",
      "9 -295.27322800317876\n"
     ]
    }
   ],
   "source": [
    "for sess in range(10):\n",
    "    batch = {\n",
    "        \"observations\": [],\n",
    "        \"actions\": [],\n",
    "        \"discounted_rewards\": []\n",
    "    }\n",
    "\n",
    "    episode_iteration = 1\n",
    "    episode_iterations = 100\n",
    "\n",
    "    total_rewards = []\n",
    "    while episode_iteration <= episode_iterations or len(batch['observations']) > 0:\n",
    "        observations, actions, rewards, dones, infos = session.run(agent, env)\n",
    "\n",
    "        batch['observations'] += observations\n",
    "        batch['actions'] += actions\n",
    "        batch['discounted_rewards'] += get_discounted_rewards(rewards, agent.gamma)\n",
    "        episode_iteration += 1\n",
    "\n",
    "        if len(batch['observations']) >= agent.batch_size:\n",
    "            for epoch in range(agent.epochs):\n",
    "                agent.learn(**batch)\n",
    "            batch = {\n",
    "                \"observations\": [],\n",
    "                \"actions\": [],\n",
    "                \"discounted_rewards\": []\n",
    "            }\n",
    "            total_rewards.append(np.sum(rewards))\n",
    "\n",
    "    print(sess, np.mean(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-550.8192623164783\n"
     ]
    }
   ],
   "source": [
    "batch = {\n",
    "    \"observations\": [],\n",
    "    \"actions\": [],\n",
    "    \"discounted_rewards\": []\n",
    "}\n",
    "\n",
    "episode_iteration = 1\n",
    "episode_iterations = 100\n",
    "\n",
    "total_rewards = []\n",
    "while episode_iteration <= episode_iterations or len(batch['observations']) > 0:\n",
    "    observations, actions, rewards, dones, infos = session.run(agent, env)\n",
    "\n",
    "    batch['observations'] += observations\n",
    "    batch['actions'] += actions\n",
    "    batch['discounted_rewards'] += get_discounted_rewards(rewards, agent.gamma)\n",
    "    episode_iteration += 1\n",
    "\n",
    "    if len(batch['observations']) >= agent.batch_size:\n",
    "        batch = {\n",
    "            \"observations\": [],\n",
    "            \"actions\": [],\n",
    "            \"discounted_rewards\": []\n",
    "        }\n",
    "        total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(np.mean(total_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rethinking Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Markov Decision Process, we have an agent and an environment who interact through actions, observations, and rewards.  The process typically works as follows:\n",
    "\n",
    "    1) The environment outputs the current state's observation\n",
    "    2) The agent receives the observation for the current state, and responds by returning an action\n",
    "    3) The environment receives the action, and returns the observation for the next state as well as the reward for the action taken during the previous state\n",
    "\n",
    "This process involves the agent taking one of some number of actions at any given state.  The action that's chosen at a given state is determined by the agent's policy.  For the sake of simplicity, we'll assume the agent is an Actor-Critic agent, so that it's policy as a probability distrubution over the possible actions.  The agent recieves the observation, and the probabilities for each action (ideally) correspond to the advantage of each action.\n",
    "\n",
    "For example, if at some state the agent's policy returns `[0.3, 0.5, 0.2]`, then the action corresponding to `0.5` is the action that should lead to the highest advantage.  If we sample from this distrubution, then we expect the agent to select the action with the highest probability the most frequently (this solves the exploration/exploitation issue to some degree).\n",
    "\n",
    "Now, during training, the agent is ran through a simulation in which it performs actions and receives rewards so as to find how those actions performed.  If the agent takes an action that results in a positive advantage, the agent is encouraged to take the action more often.  In practice, this occurs by calculating the advantage for taking that action at the state (possible since we'd have simulated the action reward results at this point) and using it to adjust the probabilities using cross-entropy loss in a neural network.\n",
    "\n",
    "In terms of training, the \"predicted\" value is equal to the masked action vector multiplied by the advantage.  For example, if at some state the action probability is `[0.3, 0.5, 0.2]` and the agent chooses the second action (whose probability is `0.5`) and this results in an advantage of `+1`, then the \"predicted\" value used for training is `[0, 1, 0]`.  This will \"push\" the probability of the second action higher while pushing the other two lower.  On the other hand, if, in a similar situation, the advantage is `-1`, then the \"predicted\" value used for training would be `[0, -1, 0]`, pushing the value lower (note: these values are negative logged, so the actual loss is `-log([0, 1, 0])` in the first case).\n",
    "\n",
    "This works well, but is not optimal for every process.  For example, by \"masking\" the other two possible actions, we are telling the agent that their advantages are 0.  This isn't necessarily true.  Consider an example where the advantage for the second and the third actions are the same (say, `+1`).  Then we are throwing away this information and training the agent on what is essentially incorrect data.  Of course, during training, we expect the agent to sample from these actions, allowing it find that these two advantages are similar and adjusting it's values accordingly.\n",
    "\n",
    "With that being said, if we can calculate the advantage for every action, then we can (trivially) train the agent to always select the best one.  The calculation of the advantage, however, is dependent on the estimation of the value by the critic.  So, if the critic can't accurately compute the value, then we can't accurately compute the advantage.\n",
    "\n",
    "On top of this, the process of simulating the environment for every path is computationally expensive.  If we tried to simulate every sequence of actions we could take, it would take `(# of actions)^(# of steps)` step simulations to perform.  If we were to just simulate the next step, this would instead take `(# of actions) * (# of steps)`.\n",
    "\n",
    "So, we can improve the accuracy of the critic by making it's task easier.  The future discounted reward is calculated as $V_t = r_t + \\gamma * V_{t+1}$.  We set $\\gamma$ to correspond to the uncertainty of future rewards, which, in typical applications, is set to 0.99.  This, to me, stands out as problematic.  With $\\gamma$ being 0.99, we giving a fair amount of weight to states that are so far out we'd have no chance of accurately predicting it in realistic environments.  The value of 0.99 has worked well in more deterministic environments, such as Atari games or physics simulations, but it has been noted the the values predicted by the critic tend to be substantially inaccurate (despite the process working relatively well).\n",
    "\n",
    "I propose a \"smarter\" approach to calculating values.  If we scale the discounting factor to correspond to how accurate we can make future predictions, we can reduce the number of states the critic can accurately account for drastically, allowing us to simulate the environment in the branching approach proposed before.\n",
    "\n",
    "For example, let's say the value is directly dependent on our ability to forecast hourly accumulation.  Let's say that the forecast produces as Gaussian distubution, with a typical variance of 1\".  So, if we forecast the accumulation to be 3\" in the next hour, we're likely to see accumulation in the range of 2\" to 4\".  This means that if we compute the value based on these forecasts, we can very rapidly find ourselves estimating inaccurate values, e.g., at only two steps out, the accumulation can either be from 1\" to 5\", at 3 steps it's 0\" to 6\", and so on (assuming high enough variance).\n",
    "\n",
    "In this case, the critic is going to do a poor job prediciting the value of the current state if we try to take into account too many steps out.  If, instead, we limit the value to only consider a few steps (either by making $\\gamma$ small or possibly a more complex function of the state $\\Gamma$), then we can not only increase the accuracy of the critic, but we can also simulate steps in the environment for every given action without the time complexity blowing up.\n",
    "\n",
    "For example, if we have 5 actions and we are confident in the critic predicting the value over the next 5 steps, then we can run the simulation at every state for every action with $5^5 = 3125$ step simulations.  This is not computationally easy, but it is feasible, and although this would be costly to run over the number of episodes typically required to train an RL agent, we will see vast improvements in the quality of training per episode, possibly offsetting the added cost to some degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the Bottom Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work through the changes required to train a successful allocation agent, we have to consider the differences in a typical Markov Decision Process and the new Allocation Process.\n",
    "\n",
    "First, consider a normal MDP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://billy-inn.github.io/images/rl1.1.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the state at timestep t $S_t$ is an observation, or a tensor of some predetermined shape which includes information about the given state which the Agent is expected to make inferences from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "http://billy-inn.github.io/images/rl1.1.png\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster",
   "language": "python",
   "name": "disaster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
